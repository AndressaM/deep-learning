\input{../header.tex}

\subtitle{The XOR Problem}

\begin{document}

\frame{\titlepage}

\section{The XOR Problem}


\begin{frame}
\frametitle{The XOR Problem}
\begin{itemize}
\item ``\textit{Perceptrons}'' by Marvin Minsky and Seymour Papert (1969).
\item Perceptrons cannot solve the XOR problem.
\item Significant decline in interest and funding of neural network research.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{The XOR Problem}
\begin{center}
\includegraphics[scale=.4]{xor-a.png}
\end{center}
\end{frame}



\begin{frame}
\frametitle{Rectified Linear Activation}
\begin{center}
\includegraphics[width=.6\textwidth]{relu.png}
\end{center}
\end{frame}




\begin{frame}
\frametitle{Network Diagrams}
\begin{columns}
\begin{column}{.4\textwidth}
\centering
\begin{center}
\includegraphics[scale=.2]{network-diagrams.png}
\end{center}
$
\textbf{h} = \max(0, \textbf{W}^{T}\textbf{x} + \textbf{c})
$\\
$
f(\textbf{x}; (\textbf{W}; \textbf{c}); (\textbf{w}, b)) = \textbf{w}^{T} \textbf{h} + b
$
\end{column}
\begin{column}{.6\textwidth}
\end{column}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Solving XOR}
\begin{columns}
\begin{column}{.4\textwidth}
\centering
\begin{center}
\includegraphics[scale=.2]{network-diagrams.png}
\end{center}
$
\textbf{h} = \max(0, \textbf{W}^{T}\textbf{x} + \textbf{c})
$\\
$
f(\textbf{x}; (\textbf{W}; \textbf{c}); (\textbf{w}, b)) = \textbf{w}^{T} \textbf{h} + b
$
\end{column}
\begin{column}{.6\textwidth}
$
X = \left [
\textbf{x}
\right ]_{i=1}^{4}
=
\left [
\begin{array}{c}
x_1 \\
x_2
\end{array}
\right ]
=
\left [
\begin{array}{cccc}
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 1
\end{array}
\right ]
$\\
$
W = 
\left [
\begin{array}{cc}
1 & 1 \\
1 & 1
\end{array}
\right ]
$\\
$
\textbf{c} =
\left [
\begin{array}{c}
0 \\
-1
\end{array}
\right ]
$\\
$
\textbf{w} =
\left [
\begin{array}{c}
1 \\
-2
\end{array}
\right ]
$\\
$
b = 0
$
\end{column}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Solving XOR}
\begin{columns}
\begin{column}{.4\textwidth}
\centering
\begin{center}
\includegraphics[scale=.2]{network-diagrams.png}
\end{center}
$
\textbf{h} = \max(0, \textbf{W}^{T}\textbf{x} + \textbf{c})
$\\
$
f(\textbf{x}; (\textbf{W}; \textbf{c}); (\textbf{w}, b)) = \textbf{w}^{T} \textbf{h} + b
$
\end{column}
\begin{column}{.6\textwidth}
$
H = 
\max \left (
0,
\textbf{W}^T X + \textbf{c}
\right )
$\\

$
H = 
\max \left (0, 
\left [
\begin{array}{cc}
1 & 1 \\
1 & 1
\end{array}
\right ]
\left [
\begin{array}{cccc}
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 1
\end{array}
\right ]
+
\left [
\begin{array}{c}
0 \\
-1
\end{array}
\right ]
\right )
$\\
$
H = 
\max \left (0, 
\left [
\begin{array}{cccc}
0 & 1 & 1 & 2 \\
0 & 1 & 1 & 2
\end{array}
\right ]
+
\left [
\begin{array}{c}
0 \\
-1
\end{array}
\right ]
\right )
$\\
%$
%H = 
%\max \left (0, 
%\left [
%\begin{array}{cccc}
%0 & 1 & 1 & 2 \\
%-1 & 0 & 0 & 1
%\end{array}
%\right ]
%\right )
%$\\
$
H = 
\max \left (0, 
\left [
\begin{array}{cccc}
0 & 1 & 1 & 2 \\
-1 & 0 & 0 & 1
\end{array}
\right ]
\right )
$\\
$
H =
\left [
\begin{array}{cccc}
0 & 1 & 1 & 2 \\
0 & 0 & 0 & 1
\end{array}
\right ]
$
\end{column}
\end{columns}
\end{frame}

%\begin{frame}
%
%\end{frame}

\begin{frame}
\begin{center}
\includegraphics[scale=.35]{h.png}
\end{center}
\end{frame}


\begin{frame}
\begin{columns}[t]
\begin{column}{.4\textwidth}
\begin{center}
\includegraphics[scale=.2]{network-diagrams.png}
\end{center}
$
\textbf{h} = \max(0, \textbf{W}^{T}\textbf{x} + \textbf{c})
$\\
$
f(\textbf{x}; (\textbf{W}; \textbf{c}); (\textbf{w}, b)) = \textbf{w}^{T} \textbf{h} + b
$
\end{column}
\begin{column}{.6\textwidth}
\[
Y = \max
\left (
0,
\textbf{w}^T H + \textbf{b}
\right )
\]
\[
Y = \max
\left (
\left [
\begin{array}{cc}
1 & -2
\end{array}
\right ]
\left [
\begin{array}{cccc}
0 & 1 & 1 & 2 \\
0 & 0 & 0 & 1
\end{array}
\right ]
+
\left [
\begin{array}{c}
0 \\
0
\end{array}
\right ]
\right )
\]
\[
Y =
\left [
\begin{array}{cccc}
0 & 1 & 1 & 0
\end{array}
\right ]
\]
\end{column}
\end{columns}
\end{frame}

%\begin{frame}
%Rectified Linear Activation (ReLU)
%\begin{itemize}
%\item Applying this function to the output of a linear transformation yields a nonlinear transformation.
%\item Very close to linear.
%\item Very simple nonlinearity (2 pieces -- piecewise-linear).
%\item Sufficient to represent any function if enough hidden units are connected.
%\item Default activation function recommended for use with most feedforward NNs.
%\end{itemize}
%Why ReLU is so effective?
%\begin{itemize}
%\item Strong gradient. Gradient descent can compute large gradients.
%\item Consistent behavior across its whole domain.
%\item Historical reasons.
%\end{itemize}
%\end{frame}



\input{../lastframe.tex}


\end{document}
